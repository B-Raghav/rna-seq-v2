# RNA-seq Visual Assistant

A local RAG chatbot + Python visualization runner for RNA-seq analysis.

## ğŸš€ Features
- PDF-based Retrieval-Augmented Generation
- Local LLM: Ollama + Mistral
- Generates executable Python visualization code
- Run plots inside the chat UI
- Windows + Mac supported

## ğŸ›  Installation

### 1. Install Ollama

Download from https://ollama.com

```bash
ollama pull mistral
```

### 2. Backend Setup

**Mac:**
```bash
cd backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

**Windows (PowerShell):**
```powershell
cd backend
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

**Windows (CMD):**
```cmd
cd backend
python -m venv venv
venv\Scripts\activate.bat
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

### 3. Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

Open http://localhost:5173

## ğŸ§ª API Endpoints
- GET /health
- POST /chat
- POST /run-code

## ğŸ“ Project Structure
```
rna-seq-v2/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py        # FastAPI app
â”‚   â”‚   â”œâ”€â”€ config.py      # Configuration
â”‚   â”‚   â”œâ”€â”€ rag.py         # RAG pipeline
â”‚   â”‚   â”œâ”€â”€ code_executor.py # Python sandbox
â”‚   â”‚   â”œâ”€â”€ models.py      # Pydantic models
â”‚   â”‚   â””â”€â”€ utils.py       # Utilities
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ rna_seq_tutorial.pdf
â”‚   â”œâ”€â”€ vectorstore/       # ChromaDB (auto-created)
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ package.json
â””â”€â”€ README.md
```

## ğŸ§¬ How It Works
1. PDF is loaded and split into chunks
2. SentenceTransformers embeds chunks
3. ChromaDB stores and retrieves relevant context
4. Ollama Mistral generates answers and Python code
5. Code runs in a sandboxed executor with matplotlib

## âš ï¸ Troubleshooting

**Windows: "python not found"**
- Use `python3` or ensure Python is in your PATH

**"Unable to fetch answer"**
- Make sure Ollama is running: `ollama serve`
- Check backend is on port 8000

**Matplotlib errors on Mac**
- Already fixed: uses 'Agg' backend

## ğŸ“œ License
MIT
