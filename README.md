# RNA-seq Visual Assistant

A local RAG chatbot + Python visualization runner.

## ðŸš€ Features
- PDF-based Retrieval-Augmented Generation
- Local LLM: Ollama + Mistral
- Generates executable Python visualization code
- Run plots inside the chat UI
- Windows + Mac supported
- Zero LangChain dependency â†’ super stable

## ðŸ›  Installation

### 1. Install Ollama

https://ollama.com

```bash
ollama pull mistral
```

### 2. Backend Setup

```bash
cd backend
python -m venv venv
# Windows
venv\\Scripts\\activate
# Mac
source venv/bin/activate

pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

### 3. Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

Open http://localhost:5173

## ðŸ§ª API Testing
- GET /health
- POST /chat
- POST /run-code

## ðŸ§± Project Structure
```
rna-seq-visual-assistant/
â”œâ”€ backend/
â”‚  â”œâ”€ app/
â”‚  â”‚  â”œâ”€ main.py
â”‚  â”‚  â”œâ”€ config.py
â”‚  â”‚  â”œâ”€ rag.py
â”‚  â”‚  â”œâ”€ code_executor.py
â”‚  â”‚  â”œâ”€ models.py
â”‚  â”‚  â”œâ”€ utils.py
â”‚  â”‚  â””â”€ __init__.py
â”‚  â”œâ”€ data/
â”‚  â”‚  â””â”€ rna_seq_tutorial.pdf
â”‚  â”œâ”€ vectorstore/
â”‚  â”‚  â””â”€ chroma/
â”‚  â”œâ”€ requirements.txt
â”‚  â””â”€ README.md
â”œâ”€ frontend/
â”‚  â””â”€ README.md
â””â”€ README.md
```

## ðŸ§¬ RAG Pipeline Description
- Load PDF
- Split into 500â€“800 char overlapping chunks
- Embed via SentenceTransformers (`all-MiniLM-L6-v2`)
- Persist in ChromaDB (`backend/vectorstore/chroma`)
- Retrieve relevant chunks for each query
- Build system prompt enforcing PDF-only answers + self-contained Python code
- Call Ollama Mistral via `POST /api/chat`
- Return Markdown answer plus cited chunks

## âœ¨ Acknowledgments
PDF source: *Unlocking Biological Insights: A Data Science Primer for RNA-seq Analysis*

## ðŸ”’ Disclaimer
Python code execution is sandboxed but not secure for untrusted users. Use only locally.

## ðŸ“œ License
MIT
